{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Guide to Image Filtering Using Convolution in OpenCV\n",
    "---\n",
    "---\n",
    "This notebook provides an in-depth exploration of image filtering techniques using convolution in OpenCV. It is inspired by and expands upon the concepts from the website: https://learnopencv.com/image-filtering-using-convolution-in-opencv/#intro-convo-kernels.\n",
    "\n",
    "We cover fundamental to advanced topics, including theoretical explanations, mathematical foundations, practical implementations, and real-world applications. Each section includes detailed documentation, code examples, and visual comparisons.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Install required libraries: `pip install opencv-python matplotlib numpy`.\n",
    "- Replace 'testImage.jpg' with your own image path if needed.\n",
    "- This notebook uses Matplotlib for displaying images in subplots for side-by-side comparisons.\n",
    "\n",
    "**Note:** All code cells assume the image is loaded in BGR format (OpenCV default). For grayscale operations, conversions are handled where necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "---\n",
    "1. Introduction to Convolution in Image Processing\n",
    "2. Mathematical Foundations of Convolution\n",
    "3. Kernel Design Principles\n",
    "4. Border Handling and Padding in Convolution\n",
    "5. Applying Identity Kernel\n",
    "6. Low-Pass Filters: Blurring Techniques\n",
    "   - Custom Box Blur\n",
    "   - Built-in Box Blur\n",
    "   - Gaussian Blur\n",
    "   - Median Blur\n",
    "   - Bilateral Filter\n",
    "7. High-Pass Filters: Sharpening and Edge Enhancement\n",
    "   - Basic Sharpening Kernel\n",
    "   - Unsharp Masking\n",
    "   - Laplacian Filter for Edge Detection\n",
    "   - Sobel Operators\n",
    "8. Directional and Advanced Kernels\n",
    "   - Emboss Filter\n",
    "   - Prewitt Operator\n",
    "   - Scharr Operator\n",
    "   - Gabor Filters\n",
    "9. Frequency Domain Filtering\n",
    "   - Fourier Transform Basics\n",
    "   - High-Pass and Low-Pass in Frequency Domain\n",
    "10. Noise in Images: Addition and Removal\n",
    "11. Performance Considerations and Optimization\n",
    "12. Real-World Applications and Case Studies\n",
    "13. Summary and Best Practices\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Convolution in Image Processing\n",
    "---\n",
    "\n",
    "Convolution is a fundamental operation in image processing where a kernel (small matrix) slides over the image, performing element-wise multiplication and summation to produce a filtered output.\n",
    "\n",
    "Key applications:\n",
    "- **Blurring (Low-Pass Filtering):** Reduces high-frequency components like noise or details.\n",
    "- **Sharpening (High-Pass Filtering):** Enhances edges and fine details.\n",
    "- **Edge Detection:** Identifies boundaries in images.\n",
    "- **Feature Extraction:** Used in CNNs for machine learning.\n",
    "\n",
    "In OpenCV, `cv2.filter2D()` is the core function for custom convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical Foundations of Convolution\n",
    "---\n",
    "\n",
    "### Discrete 2D Convolution\n",
    "\n",
    "For a 2D image $I$ and a kernel $K$ of size $(2m+1) \\times (2n+1)$, the convolution at pixel $(x, y)$ is defined as:\n",
    "$$\n",
    "(I * K)(x, y)\n",
    "=\n",
    "\\sum_{i=-m}^{m}\n",
    "\\sum_{j=-n}^{n}\n",
    "I(x+i, y+j)\\, K(m-i, n-j)\n",
    "$$\n",
    "### Key Concepts\n",
    "- **1D vs 2D Convolution :** 1D convolution is used for signals (audio, time-series), while 2D convolution is used for images.  \n",
    "  *Separable kernels* (e.g., Gaussian) can be decomposed into two 1D convolutions for efficiency.\n",
    "- **Normalization :** Kernels are often normalized by dividing each element by the sum of all kernel values to preserve intensity.\n",
    "- **Cross-Correlation vs Convolution :** OpenCV’s `filter2D()` applies **cross-correlation** (kernel is not flipped).\n",
    "- **symmetric kernels :** convolution and cross-correlation are mathematically equivalent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kernel Design Principles\n",
    "---\n",
    "- **Size:** Odd dimensions (e.g., 3x3, 5x5) center easily.\n",
    "- **Symmetry:** Often symmetric for isotropic effects.\n",
    "- **Sum:** For low-pass, sum=1 (normalized); for high-pass, sum=0 or 1.\n",
    "- **Examples:**\n",
    "  - Identity: No change.\n",
    "  - Box: Uniform averaging.\n",
    "  - Gaussian: Weighted averaging based on distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Border Handling and Padding in Convolution\n",
    "---\n",
    "When the kernel overlaps image edges, padding is needed. OpenCV's `borderType` in `filter2D()`:\n",
    "- `cv2.BORDER_CONSTANT`: Pad with constant value.\n",
    "- `cv2.BORDER_REPLICATE`: Repeat edge pixels.\n",
    "- `cv2.BORDER_REFLECT`: Mirror reflection.\n",
    "- `cv2.BORDER_WRAP`: Wrap around.\n",
    "- Default: `cv2.BORDER_DEFAULT` (reflect with no edge pixel flip).\n",
    "\n",
    "Example: Padding affects edge artifacts in filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all dependencies\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tools.tools import LearnTools\n",
    "\n",
    "learn_tools = LearnTools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and Load the image\n",
    "\n",
    "if os.path.exists('testImage.jpg'):\n",
    "    image = cv2.imread('testImage.jpg')\n",
    "    print('Image Exists')\n",
    "elif not os.path.exists('testImage.jpg'):\n",
    "    image_url = \"https://i.ibb.co.com/BVSYcmyY/joey-kyber-GPxgi4-J82-E4-unsplash.jpg\"\n",
    "    pil_image = await learn_tools.get_image(img_url=image_url, padding=0)\n",
    "    pil_image.save('testImage.jpg', 'JPEG')\n",
    "    # or\n",
    "    image = learn_tools.pil_to_cv2(pil_image=pil_image)\n",
    "    # cv2.imwrite('testImage.jpg', image)\n",
    "    print('Image Created')\n",
    "\n",
    "if image is not None:\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    # Display the images using LearnTools\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Original Image','image': image},\n",
    "            {'title': 'Gray Image','image': gray_image}\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    print('The image could not be loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applying Identity Kernel\n",
    "The identity kernel preserves the original image:\n",
    "$$\n",
    "K =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "This demonstrates basic convolution without modifying the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What `ddepth` actually means?\n",
    "\n",
    "`ddepth` = destination depth\n",
    "\n",
    "It defines the data type of the output image, not the kernel.\n",
    "\n",
    "Examples of depths:\n",
    "- `cv2.CV_8U` → `uint8`\n",
    "- `cv2.CV_16S` → `int16`\n",
    "- `cv2.CV_32F` → `float32`\n",
    "- `cv2.CV_64F` → `float64`\n",
    "\n",
    "`ddepth` Use cases:\n",
    "- `cv2.CV_32F` → Make it float\n",
    "- `cv2.CV_16S` → Use signed 16-bit\n",
    "- `-1` → Don’t decide. Just reuse the input type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image is not None:\n",
    "    # Step 1: Define the identity kernel\n",
    "    kernel_identity = np.array(\n",
    "        [\n",
    "            [0, 0, 0],\n",
    "            [0, 1, 0],\n",
    "            [0, 0, 0]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Step 2: Apply convolution using the identity kernel\n",
    "    identity_img = cv2.filter2D(src=image, ddepth=-1, kernel=kernel_identity)\n",
    "\n",
    "    # Display the Original Image and Identity Image using LearnTools\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Original Image', 'image': image},\n",
    "            {'title': 'Identity Filtered Image', 'image': identity_img}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optional: Save the resulting image to disk\n",
    "    # cv2.imwrite('Identity_Image.jpg', identity_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Low-Pass Filters: Blurring Techniques\n",
    "\n",
    "Low-pass filters attenuate high-frequency components, resulting in a smoother image.\n",
    "\n",
    "### 6.1 Custom Box Blur\n",
    "\n",
    "A box blur uses a **normalized averaging kernel**:\n",
    "\n",
    "$$\n",
    "K = \\frac{1}{50}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\quad (8 \\times 8)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image is not None:\n",
    "    # Step 1: Define a custom 5x5 box blur kernel\n",
    "    kernel_box = np.ones((8, 8), np.float32) / 50\n",
    "\n",
    "    # Step 2: Apply convolution with the box blur kernel\n",
    "    box_blur_custom = cv2.filter2D(src=image, ddepth=-1, kernel=kernel_box)\n",
    "\n",
    "    # Step 3: Display the original and blurred images side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Original Image', 'image': image},\n",
    "            {'title': 'Custom Box Blurr Image', 'image': box_blur_custom}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Step 4: Save the blurred image to disk\n",
    "    # cv2.imwrite('custom_box_blur_image.jpg', box_blur_custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Built-in Box Blur\n",
    "\n",
    "OpenCV's `cv2.blur()` is optimized for box filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image is not None:\n",
    "    # Step 1: Apply OpenCV's built-in box blur function\n",
    "    box_blur_builtin = cv2.blur(src=image, ksize=(8, 8))\n",
    "\n",
    "    # Step 2: Display the original and blurred images side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Original Image', 'image': image},\n",
    "            {'title': 'Built-in Box Blur Image', 'image': box_blur_builtin}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optional: Save the resulting image\n",
    "    # cv2.imwrite('box_blur_builtin.jpg', box_blur_builtin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Gaussian Blur\n",
    "\n",
    "Gaussian blur uses a kernel based on the **Gaussian (normal) distribution**.  \n",
    "It smooths the image while preserving edges better than a simple box blur.\n",
    "\n",
    "- **Sigma (σ)** controls the **spread of the Gaussian**:  \n",
    "  - Larger σ → more blur  \n",
    "  - Smaller σ → less blur\n",
    "\n",
    "The 2D Gaussian function is defined as:\n",
    "\n",
    "$$\n",
    "G(x, y) = \\frac{1}{2 \\pi \\sigma^2} \\, e^{-\\frac{x^2 + y^2}{2 \\sigma^2}}\n",
    "$$\n",
    "\n",
    "- Here, \\(x\\) and \\(y\\) are the distances from the kernel center.  \n",
    "- The kernel weights decrease with distance from the center, giving a **weighted average** that emphasizes nearby pixels.\n",
    "\n",
    "**Key Notes:**\n",
    "1. Gaussian blur is a **low-pass filter**, reducing high-frequency noise.  \n",
    "2. Unlike a box blur, it gives **smoother, visually pleasing results** because edges are less harshly averaged.  \n",
    "3. In OpenCV, you can apply it using `cv2.GaussianBlur(image, ksize, sigmaX)`.\n",
    "\n",
    "**Parameter breakdown**\n",
    "`src=image`\n",
    "- Input image (grayscale or color).\n",
    "- Must be a NumPy array.\n",
    "- If it’s noisy, this operation reduces that noise.\n",
    "\n",
    "`ksize=(5, 5)`\n",
    "- Size of the convolution kernel.\n",
    "- Must be odd and positive.\n",
    "- (5,5) means each pixel is recalculated using a 5×5 neighborhood.\n",
    "- Larger kernel → stronger blur → more detail loss.\n",
    "\n",
    "`Quick intuition:`\n",
    "    - (3,3) → light smoothing\n",
    "    - (5,5) → moderate blur ✅ (common default)\n",
    "    - (9,9) → heavy blur\n",
    "\n",
    "`sigmaX=0`\n",
    "- Standard deviation in X direction.\n",
    "- 0 means OpenCV auto-computes sigma from kernel size.\n",
    "- For (5,5), OpenCV picks a reasonable sigma so you don’t have to tune it.\n",
    "***(Internally: `sigma ≈ 0.3*((ksize−1)*0.5 − 1) + 0.8`)***\n",
    "\n",
    "| Kernel  | Typical Sigma |\n",
    "| ------- | ------------- |\n",
    "| (3,3)   | ~0.8          |\n",
    "| (5,5)   | ~1.2          |\n",
    "| (9,9)   | ~1.8          |\n",
    "| (15,15) | ~3.0          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image is not None:\n",
    "    # Step 1: Apply Gaussian blur\n",
    "    gaussian_blur = cv2.GaussianBlur(src=image, ksize=(15, 15), sigmaX=3)\n",
    "\n",
    "    # Step 2: Display the original and blurred images side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Original Image', 'image': image},\n",
    "            {'title': 'Gaussian Blur Image', 'image': gaussian_blur}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optional: Save the blurred image to disk\n",
    "    # cv2.imwrite('gaussian_blur.jpg', gaussian_blur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Median Blur\n",
    "Median blur is a non-linear spatial filter that replaces each pixel value with the median of the surrounding neighborhood.Unlike Gaussian blur, it does not average values, which makes it highly effective at removing impulse (salt-and-pepper) noise while preserving edges.\n",
    "\n",
    "**How Median Blur Works**\n",
    "For each pixel:\n",
    "- Collect neighboring pixel values defined by the kernel\n",
    "- Sort the values\n",
    "- Replace the center pixel with the median value\n",
    "\n",
    "No weighting. No convolution. Just statistics.Mathematically, for a neighborhood `W`:\n",
    "$$\n",
    "I′(x,y)=median{I(i,j)∣(i,j)∈W}\n",
    "$$\n",
    "\n",
    "**Why Median Instead of Mean**\n",
    "- Median rejects outliers\n",
    "- Sharp intensity spikes (0 or 255) are eliminated\n",
    "- Preserves edges better than linear filters\n",
    "\n",
    "***Key Notes***\n",
    "- Median blur is a non-linear filter, so it cannot be represented by convolution.\n",
    "- It is highly effective for salt-and-pepper noise.\n",
    "- Edges are preserved better than Gaussian blur.\n",
    "- Sorting makes it computationally heavier.\n",
    "- Use case : `cv2.medianBlur(image, ksize)`\n",
    "\n",
    "**Parameter Breakdown**\n",
    "`src = image`\n",
    "- Input image (grayscale or color)\n",
    "- Must be a NumPy array\n",
    "- Best suited for impulse-noise–corrupted images\n",
    "\n",
    "`ksize = 5`\n",
    "- Size of the square neighborhood\n",
    "- Must be odd and greater than 1\n",
    "- Determines the number of pixels used to compute the median\n",
    "\n",
    "***Quick Intuition***\n",
    "- 3 → removes light impulse noise, minimal detail loss ✅\n",
    "- 5 → strong salt-and-pepper noise suppression\n",
    "- 7+ → heavy smoothing, important details start disappearing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image is not None:\n",
    "    # Step 1: Apply Median blur\n",
    "    # Median blur is a non-linear filter that replaces each pixel\n",
    "    # with the median value of its neighborhood (k x k).\n",
    "    # It is especially effective for salt-and-pepper noise.\n",
    "    median_blur = cv2.medianBlur(src=image, ksize=7)\n",
    "\n",
    "    # Step 2: Display the original and median-blurred images side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Original Image', 'image': image},\n",
    "            {'title': 'Median Blur Image', 'image': median_blur}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optional: Save the median-blurred image to disk\n",
    "    # cv2.imwrite('median_blur.jpg', median_blur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Bilateral Filter\n",
    "Bilateral filter is an edge-preserving, noise-reducing filter. Unlike Gaussian blur, it considers both spatial proximity and pixel intensity differences, so edges are preserved while smoothing flat regions.\n",
    "\n",
    "**How Bilateral Filter Works**\n",
    "\n",
    "For each pixel:\n",
    "- Consider a neighborhood around the pixel.\n",
    "- Compute a weighted average where weights depend on:\n",
    "    - **Spatial distance**: nearby pixels contribute more.\n",
    "    - **Intensity difference**: pixels with similar colors contribute more.\n",
    "- Replace the pixel with the weighted average.\n",
    "\n",
    "Mathematically, for a pixel $I(x, y)$:\n",
    "\n",
    "$$\n",
    "I_{\\text{out}}(x, y) = \\frac{1}{W_p} \\sum_{(i, j) \\in N_k} \n",
    "I(i, j) \\,\n",
    "\\exp\\Bigg(-\\frac{(i - x)^2 + (j - y)^2}{2 \\sigma_s^2}\\Bigg) \n",
    "\\exp\\Bigg(-\\frac{|I(i, j) - I(x, y)|^2}{2 \\sigma_r^2}\\Bigg)\n",
    "$$\n",
    "\n",
    "where the **normalization factor** $W_p$ ensures that the weights sum to 1:\n",
    "\n",
    "$$\n",
    "W_p = \\sum_{(i, j) \\in N_k} \n",
    "\\exp\\Bigg(-\\frac{(i - x)^2 + (j - y)^2}{2 \\sigma_s^2}\\Bigg) \n",
    "\\exp\\Bigg(-\\frac{|I(i, j) - I(x, y)|^2}{2 \\sigma_r^2}\\Bigg)\n",
    "$$\n",
    "\n",
    "**Parameters:**\n",
    "- $N_k$: neighborhood of the pixel $(x, y)$  \n",
    "- $\\sigma_s$: spatial standard deviation (controls how much nearby pixels influence the filter)  \n",
    "- $\\sigma_r$: range (intensity) standard deviation (controls how much pixels with different intensities are considered)  \n",
    "\n",
    "The bilateral filter smooths regions while preserving edges, making it very effective for noise reduction without blurring edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image is not None:\n",
    "    # Apply bilateral filter\n",
    "    # d = diameter of pixel neighborhood\n",
    "    # sigmaColor = filter sigma in color space\n",
    "    # sigmaSpace = filter sigma in coordinate space\n",
    "    bilateral = cv2.bilateralFilter(src=image, d=15, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # Display original and filtered images side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Original Image', 'image': image},\n",
    "            {'title': 'Bilateral Image', 'image': bilateral}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optionally save the filtered image\n",
    "    # cv2.imwrite('bilateral.jpg', bilateral)\n",
    "else:\n",
    "    print(\"No image loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. High-Pass Filters: Sharpening and Edge Enhancement\n",
    "\n",
    "High-pass filters are used to **emphasize edges and fine details** in an image.  \n",
    "They work by **removing low-frequency components** (smooth areas) from the original image, highlighting rapid intensity changes.\n",
    "\n",
    "A simple and commonly used 3x3 sharpening kernel is:\n",
    "\n",
    "$$\n",
    "K = \n",
    "\\begin{bmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & -1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Explanation:**\n",
    "- The center value `5` amplifies the current pixel.\n",
    "- The `-1` values subtract contributions from the 4 immediate neighbors.\n",
    "- The sum of kernel coefficients is 1, preserving overall brightness.\n",
    "- This kernel enhances edges while leaving flat regions mostly unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image is not None:\n",
    "    # Define a 3x3 high-pass sharpening kernel\n",
    "    # Center value amplifies the pixel\n",
    "    # Negative neighbors subtract surrounding intensities (edge enhancement)\n",
    "    kernel_sharpen = np.array([[0, -1, 0],\n",
    "                               [-1,  5, -1],\n",
    "                               [0, -1, 0]])\n",
    "\n",
    "    # Apply convolution-based sharpening\n",
    "    # ddepth = -1 keeps the output image depth same as input\n",
    "    sharpened = cv2.filter2D(src=image, ddepth=-1, kernel=kernel_sharpen)\n",
    "\n",
    "    # Display original and sharpened images side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Original Image', 'image': image},\n",
    "            {'title': 'Sharpen Image', 'image': sharpened}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optionally save the sharpened image\n",
    "    # cv2.imwrite('sharpened.jpg', sharpened)\n",
    "else:\n",
    "    print(\"No image loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Unsharp Masking\n",
    "\n",
    "Unsharp masking is a widely used **high-pass sharpening technique** that enhances edges while preserving overall image structure.  \n",
    "Instead of applying a fixed kernel, it sharpens an image by **adding scaled high-frequency details** back to the original image.\n",
    "\n",
    "#### Principle\n",
    "\n",
    "1. Blur the original image to remove high-frequency components.\n",
    "2. Subtract the blurred image from the original to obtain edge details.\n",
    "3. Add the scaled edge details back to the original image.\n",
    "\n",
    "This enhances edges without significantly affecting smooth regions.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "Let:\n",
    "- $I_{\\text{original}}$ be the original image\n",
    "- $I_{\\text{blurred}}$ be the low-pass (blurred) image\n",
    "- $\\alpha$ be the sharpening strength\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "I_{\\text{sharpened}} =\n",
    "I_{\\text{original}} + \\alpha \\cdot\n",
    "\\left( I_{\\text{original}} - I_{\\text{blurred}} \\right)\n",
    "$$\n",
    "\n",
    "- $(I_{\\text{original}} - I_{\\text{blurred}})$ acts as a **high-pass filter**\n",
    "- $\\alpha$ controls how strong the sharpening effect is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image is not None:\n",
    "    # Apply Gaussian blur to create a low-pass version of the image\n",
    "    # (0, 0) lets OpenCV compute kernel size from sigma\n",
    "    # sigmaX controls the amount of smoothing\n",
    "    blurred = cv2.GaussianBlur(image, (0, 0), sigmaX=3)\n",
    "\n",
    "    # Apply unsharp masking\n",
    "    # 1.5 -> weight of the original image (detail amplification)\n",
    "    # -0.5 -> weight of the blurred image (detail subtraction)\n",
    "    # 0 -> scalar added to the result\n",
    "    unsharp_mask = cv2.addWeighted(image, 1.5, blurred, -0.5, 0)\n",
    "\n",
    "    # Display original and unsharp-masked images side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Original Image', 'image': image},\n",
    "            {'title': 'Unshaped Mask Image', 'image': unsharp_mask}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optionally save the sharpened image\n",
    "    # cv2.imwrite('unsharp_mask.jpg', unsharp_mask)\n",
    "else:\n",
    "    print(\"No image loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Laplacian Filter for Edge Detection\n",
    "\n",
    "The Laplacian filter is a **second-order derivative operator** used to highlight regions of rapid intensity change in an image.  \n",
    "Unlike gradient-based filters (e.g., Sobel), the Laplacian responds to edges **in all directions equally**.\n",
    "\n",
    "#### Principle\n",
    "\n",
    "- Flat regions produce values close to zero.\n",
    "- Sudden intensity transitions (edges) produce large positive or negative responses.\n",
    "- Because it is a second derivative, the Laplacian is **highly sensitive to noise**.\n",
    "\n",
    "For this reason, it is often applied after a **Gaussian blur**.\n",
    "\n",
    "#### Laplacian Kernel\n",
    "\n",
    "A commonly used 3×3 Laplacian kernel is:\n",
    "\n",
    "$$\n",
    "K =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & -4 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- The negative center emphasizes intensity discontinuities.\n",
    "- Positive neighboring values capture surrounding pixel changes.\n",
    "- The sum of coefficients is zero, making it a **true high-pass filter**.\n",
    "\n",
    "#### Mathematical Interpretation\n",
    "\n",
    "The Laplacian operator approximates second-order spatial derivatives:\n",
    "\n",
    "$$\n",
    "\\nabla^2 I = \\frac{\\partial^2 I}{\\partial x^2}\n",
    "           + \\frac{\\partial^2 I}{\\partial y^2}\n",
    "$$\n",
    "\n",
    "Large values of \\( \\nabla^2 I \\) indicate edges or fine detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gray_image is not None:\n",
    "    # Apply Laplacian filter (second-order derivative)\n",
    "    # ddepth = CV_64F allows negative values from second derivative calculation\n",
    "    # ksize = 3 defines the aperture size for the Laplacian operator\n",
    "    laplacian = cv2.Laplacian(gray_image, ddepth=cv2.CV_64F, ksize=3)\n",
    "\n",
    "    # Convert result to 8-bit absolute values for visualization\n",
    "    laplacian_abs = cv2.convertScaleAbs(laplacian)\n",
    "\n",
    "    # Display grayscale original and Laplacian edge image side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Gray Original Image', 'image': gray_image},\n",
    "            {'title': 'Laplacian Image', 'image': laplacian_abs}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optionally save the result\n",
    "    # cv2.imwrite('laplacian.jpg', laplacian_abs)\n",
    "else:\n",
    "    print(\"No grayscale image loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Sobel Operators\n",
    "\n",
    "Sobel operators compute the **first-order spatial derivative** of an image and are widely used for **edge detection**.  \n",
    "They respond strongly to edges aligned in the **horizontal** or **vertical** direction.\n",
    "\n",
    "#### Principle\n",
    "\n",
    "- Measures the gradient (rate of intensity change) in the image.\n",
    "- Produces two gradient images:\n",
    "  - $G_x$: horizontal (vertical-edge) response\n",
    "  - $G_y$: vertical (horizontal-edge) response\n",
    "- Combines both to estimate edge strength and orientation.\n",
    "\n",
    "#### Sobel Kernels\n",
    "\n",
    "**Horizontal Gradient ($K_x$):**\n",
    "\n",
    "$$\n",
    "K_x =\n",
    "\\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-2 & 0 & 2 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Vertical Gradient ($K_y$):**\n",
    "\n",
    "$$\n",
    "K_y =\n",
    "\\begin{bmatrix}\n",
    "-1 & -2 & -1 \\\\\n",
    "0  &  0 &  0 \\\\\n",
    "1  &  2 &  1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Coefficients emphasize intensity differences across directions.\n",
    "- Larger center weights improve noise robustness compared to simple gradient filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gray_image is not None:\n",
    "    # Compute Sobel gradient in the x-direction (vertical edges)\n",
    "    # CV_64F is used to preserve negative gradient values\n",
    "    # dx = 1, dy = 0 → horizontal derivative\n",
    "    sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, dx=1, dy=0, ksize=3)\n",
    "\n",
    "    # Compute Sobel gradient in the y-direction (horizontal edges)\n",
    "    # dx = 0, dy = 1 → vertical derivative\n",
    "    sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, dx=0, dy=1, ksize=3)\n",
    "\n",
    "    # Combine x and y gradients to compute edge magnitude\n",
    "    sobel_combined = cv2.magnitude(sobel_x, sobel_y)\n",
    "\n",
    "    # Convert to 8-bit absolute values for display purposes\n",
    "    sobel_combined_abs = cv2.convertScaleAbs(sobel_combined)\n",
    "\n",
    "    # Display original grayscale image and Sobel edge map side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Gray Original Image', 'image': gray_image},\n",
    "            {'title': 'Sobel Edge Image', 'image': sobel_combined_abs}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optionally save the result\n",
    "    # cv2.imwrite('sobel.jpg', sobel_combined_abs)\n",
    "else:\n",
    "    print(\"No grayscale image loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Directional and Advanced Kernels\n",
    "\n",
    "Directional kernels emphasize edges and textures in **specific directions**.  \n",
    "They are often used for **stylized effects**, feature enhancement, or simulating lighting.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.1 Emboss Filter\n",
    "\n",
    "The emboss filter creates a **3D-like relief effect** by simulating directional lighting across image edges.  \n",
    "It highlights intensity transitions in a specific direction while suppressing others, giving the illusion of depth.\n",
    "\n",
    "---\n",
    "\n",
    "#### Principle\n",
    "\n",
    "- Enhances edges by exaggerating differences along a chosen direction.\n",
    "- Shifts pixel intensities to simulate light coming from one side.\n",
    "- Produces bright and dark regions resembling raised and recessed surfaces.\n",
    "\n",
    "---\n",
    "\n",
    "#### Emboss Kernel\n",
    "\n",
    "A commonly used 3×3 emboss kernel is:\n",
    "\n",
    "$$\n",
    "K =\n",
    "\\begin{bmatrix}\n",
    "-2 & -1 & 0 \\\\\n",
    "-1 &  1 & 1 \\\\\n",
    " 0 &  1 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Negative values suppress pixels on one side of the edge.\n",
    "- Positive values enhance pixels on the opposite side.\n",
    "- The kernel direction defines the **apparent light source direction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image is not None:\n",
    "    # Define a 3x3 emboss kernel\n",
    "    # Negative values darken pixels on one side of the edge\n",
    "    # Positive values brighten pixels on the opposite side\n",
    "    kernel_emboss = np.array([\n",
    "        [-2, -1, 0],\n",
    "        [-1,  1, 1],\n",
    "        [ 0,  1, 2]\n",
    "    ])\n",
    "\n",
    "    # Apply convolution to create emboss effect\n",
    "    # ddepth = -1 keeps original image depth\n",
    "    emboss = cv2.filter2D(src=image, ddepth=-1, kernel=kernel_emboss)\n",
    "\n",
    "    # Display original and embossed images side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Original Image', 'image': image},\n",
    "            {'title': 'Emboss Image', 'image': emboss}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optionally save the result\n",
    "    # cv2.imwrite('emboss.jpg', emboss)\n",
    "else:\n",
    "    print(\"No image loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Prewitt Operator\n",
    "\n",
    "The Prewitt operator is a simple **first-order derivative filter** used for edge detection.  \n",
    "It is similar to the Sobel operator but **without weighting the center row/column**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Principle\n",
    "\n",
    "- Measures the gradient in an image to detect edges.\n",
    "- Produces horizontal and vertical responses:\n",
    "  - $G_x$: detects vertical edges (changes along x)\n",
    "  - $G_y$: detects horizontal edges (changes along y)\n",
    "- Less smooth than Sobel but simpler to compute.\n",
    "\n",
    "---\n",
    "\n",
    "#### Prewitt Kernels\n",
    "\n",
    "**Horizontal Gradient ($K_x$):**\n",
    "\n",
    "$$\n",
    "K_x =\n",
    "\\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Vertical Gradient ($K_y$):**\n",
    "\n",
    "$$\n",
    "K_y =\n",
    "\\begin{bmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "0  &  0 &  0 \\\\\n",
    "1  &  1 &  1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Unlike Sobel, all rows/columns are equally weighted.\n",
    "- Simpler and faster but more sensitive to noise.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gradient Magnitude\n",
    "\n",
    "The edge strength is computed as:\n",
    "\n",
    "$$\n",
    "G = \\sqrt{G_x^2 + G_y^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gray_image is not None:\n",
    "    # Define the horizontal Prewitt kernel\n",
    "    # Detects vertical edges (changes along the x-axis)\n",
    "    kernel_prewitt_x = np.array([\n",
    "        [-1, 0, 1],\n",
    "        [-1, 0, 1],\n",
    "        [-1, 0, 1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Apply convolution to extract horizontal gradient\n",
    "    prewitt_x = cv2.filter2D(gray_image, ddepth=-1, kernel=kernel_prewitt_x)\n",
    "\n",
    "    # Display grayscale original and Prewitt X edge image side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Gray Original Image', 'image': gray_image},\n",
    "            {'title': 'Prewitt X Image', 'image': prewitt_x}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optionally save the result\n",
    "    # cv2.imwrite('prewitt.jpg', prewitt_x)\n",
    "else:\n",
    "    print(\"No grayscale image loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Scharr Operator\n",
    "\n",
    "The Scharr operator is an **optimized version of the Sobel operator**, designed to improve **rotation invariance** and edge detection accuracy, especially for small kernels (3×3).\n",
    "\n",
    "---\n",
    "\n",
    "#### Principle\n",
    "\n",
    "- Computes the **first-order derivative** of the image, like Sobel.\n",
    "- Uses **larger weights** for the center pixels to reduce rotational artifacts.\n",
    "- Provides **better edge detection** for diagonal and small-scale features compared to Sobel.\n",
    "\n",
    "---\n",
    "\n",
    "#### Scharr Kernels\n",
    "\n",
    "**Horizontal Gradient ($K_x$):**\n",
    "\n",
    "$$\n",
    "K_x =\n",
    "\\begin{bmatrix}\n",
    " 3 & 0 & -3 \\\\\n",
    "10 & 0 & -10 \\\\\n",
    " 3 & 0 & -3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Vertical Gradient ($K_y$):**\n",
    "\n",
    "$$\n",
    "K_y =\n",
    "\\begin{bmatrix}\n",
    " 3 & 10 & 3 \\\\\n",
    " 0 &  0 & 0 \\\\\n",
    "-3 & -10 & -3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- The larger weights (10 vs 2 in Sobel) improve sensitivity to edges and reduce rotational bias.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gradient Magnitude\n",
    "\n",
    "The overall edge strength is computed as:\n",
    "\n",
    "$$\n",
    "G = \\sqrt{G_x^2 + G_y^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gray_image is not None:\n",
    "    # Compute Scharr gradient in x-direction (vertical edges)\n",
    "    # CV_64F preserves negative values from derivative calculation\n",
    "    # dx=1, dy=0 → horizontal derivative\n",
    "    scharr_x = cv2.Scharr(gray_image, ddepth=cv2.CV_64F, dx=1, dy=0)\n",
    "\n",
    "    # Convert result to 8-bit absolute values for display\n",
    "    scharr_abs = cv2.convertScaleAbs(scharr_x)\n",
    "\n",
    "    # Display original grayscale and Scharr X edge image side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Gray Original Image', 'image': gray_image},\n",
    "            {'title': 'Scharr X Image', 'image': scharr_abs}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optionally save the result\n",
    "    # cv2.imwrite('scharr.jpg', scharr_abs)\n",
    "else:\n",
    "    print(\"No grayscale image loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Gabor Filters\n",
    "\n",
    "Gabor filters are **frequency- and orientation-selective** filters widely used for **texture analysis, feature extraction, and edge detection**.  \n",
    "They model the response of **visual cortical cells** and are sensitive to specific spatial frequencies and orientations.\n",
    "\n",
    "---\n",
    "\n",
    "#### Principle\n",
    "\n",
    "- Combines a **Gaussian envelope** with a **sinusoidal plane wave**.\n",
    "- Responds strongly to edges or textures aligned with its orientation.\n",
    "- Useful for detecting **periodic patterns, textures, and directional features**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gabor Kernel\n",
    "\n",
    "Mathematically, a 2D Gabor kernel is defined as:\n",
    "\n",
    "$$\n",
    "g(x, y) = \\exp\\Bigg(-\\frac{x'^2 + \\gamma^2 y'^2}{2\\sigma^2}\\Bigg)\n",
    "          \\cos\\Big(2\\pi \\frac{x'}{\\lambda} + \\psi \\Big)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x' = x \\cos\\theta + y \\sin\\theta$  \n",
    "- $y' = -x \\sin\\theta + y \\cos\\theta$  \n",
    "- $\\lambda$: wavelength of the sinusoid (controls frequency)  \n",
    "- $\\theta$: orientation of the filter  \n",
    "- $\\psi$: phase offset  \n",
    "- $\\sigma$: standard deviation of Gaussian envelope  \n",
    "- $\\gamma$: spatial aspect ratio (ellipticity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gray_image is not None:\n",
    "    # Define Gabor kernel parameters\n",
    "    # ksize = size of the filter (width, height)\n",
    "    # sigma = standard deviation of Gaussian envelope\n",
    "    # theta = orientation of the filter (radians)\n",
    "    # lambd = wavelength of the sinusoidal factor\n",
    "    # gamma = spatial aspect ratio (ellipticity)\n",
    "    # psi = phase offset\n",
    "    gabor_kernel = cv2.getGaborKernel(\n",
    "        ksize=(31, 31),\n",
    "        sigma=4.0,\n",
    "        theta=np.pi/4,\n",
    "        lambd=10.0,\n",
    "        gamma=0.5,\n",
    "        psi=0\n",
    "    )\n",
    "\n",
    "    # Apply Gabor filter using convolution\n",
    "    gabor_filtered = cv2.filter2D(gray_image, ddepth=cv2.CV_8UC3, kernel=gabor_kernel)\n",
    "\n",
    "    # Display original and Gabor-filtered images side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Gray Original Image', 'image': gray_image},\n",
    "            {'title': 'Gabor Filter Image', 'image': gabor_filtered}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optionally save the filtered result\n",
    "    # cv2.imwrite('gabor.jpg', gabor_filtered)\n",
    "else:\n",
    "    print(\"No grayscale image loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Frequency Domain Filtering\n",
    "\n",
    "Frequency domain filtering leverages the **Fourier Transform** to analyze and manipulate image components based on their frequency.  \n",
    "A key principle:\n",
    "\n",
    "> Convolution in the spatial domain is equivalent to multiplication in the frequency domain.\n",
    "\n",
    "This allows **high-pass, low-pass, and band-pass filtering** to be implemented efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### 9.1 Fourier Transform Basics\n",
    "\n",
    "The **Discrete Fourier Transform (DFT)** converts an image from the **spatial domain** to the **frequency domain**:\n",
    "\n",
    "- Low frequencies: represent **slow variations**, smooth regions.\n",
    "- High frequencies: represent **rapid changes**, edges, and fine details.\n",
    "\n",
    "The 2D Fourier Transform of an image \\( I(x, y) \\) is:\n",
    "\n",
    "$$\n",
    "F(u, v) = \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} I(x, y) \\, e^{-j 2 \\pi \\left( \\frac{ux}{M} + \\frac{vy}{N} \\right)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( M, N \\) are the image dimensions.\n",
    "- \\( (u, v) \\) are frequency coordinates.\n",
    "- \\( j = \\sqrt{-1} \\) is the imaginary unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gray_image is not None:\n",
    "    # Compute the Discrete Fourier Transform (DFT)\n",
    "    dft = cv2.dft(np.float32(gray_image), flags=cv2.DFT_COMPLEX_OUTPUT)\n",
    "\n",
    "    # Shift the zero-frequency component to the center\n",
    "    dft_shift = np.fft.fftshift(dft)\n",
    "\n",
    "    # Compute magnitude spectrum for visualization\n",
    "    magnitude_spectrum = 20 * np.log(cv2.magnitude(dft_shift[:, :, 0], dft_shift[:, :, 1]) + 1e-8)\n",
    "\n",
    "    # Display original grayscale image and its magnitude spectrum using show_two_images\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Gray Original Image', 'image': gray_image, 'cmap': 'gray'},\n",
    "            {'title': 'Magnitude Spectrum', 'image': magnitude_spectrum, 'cmap': 'gray'},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"No grayscale image loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 High-Pass and Low-Pass in Frequency Domain\n",
    "\n",
    "Frequency domain filtering allows selective enhancement or suppression of certain frequency components:\n",
    "\n",
    "- **Low-pass filter (LPF):** allows low frequencies to pass and **blocks high frequencies** → smooths the image.\n",
    "- **High-pass filter (HPF):** allows high frequencies to pass and **blocks low frequencies** → emphasizes edges and fine details.\n",
    "\n",
    "---\n",
    "\n",
    "#### Ideal Low-Pass Filter\n",
    "\n",
    "- Masks all frequencies outside a circular region of radius `D0` in the frequency domain.\n",
    "- Let `F(u, v)` be the DFT of the image. The filtered spectrum `G(u, v)`:\n",
    "\n",
    "$$\n",
    "G(u, v) =\n",
    "\\begin{cases}\n",
    "F(u, v), & \\text{if } D(u, v) \\le D_0 \\\\\n",
    "0, & \\text{if } D(u, v) > D_0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( D(u, v) = \\sqrt{(u - M/2)^2 + (v - N/2)^2} \\) is the distance from the center frequency.\n",
    "- \\( D_0 \\) is the cutoff frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gray_image is not None:\n",
    "    # Get image dimensions\n",
    "    rows, cols = gray_image.shape\n",
    "    crow, ccol = rows // 2, cols // 2  # center\n",
    "\n",
    "    # Create a circular mask for low-pass filtering\n",
    "    # All frequencies outside the central square region are set to 0\n",
    "    mask = np.zeros((rows, cols, 2), np.uint8)\n",
    "    mask[crow-30:crow+30, ccol-30:ccol+30] = 1  # square low-pass region\n",
    "\n",
    "    # Apply the mask to the shifted DFT\n",
    "    fshift = dft_shift * mask\n",
    "\n",
    "    # Shift back and compute inverse DFT\n",
    "    f_ishift = np.fft.ifftshift(fshift)\n",
    "    img_back = cv2.idft(f_ishift)\n",
    "\n",
    "    # Compute magnitude to get the filtered image\n",
    "    img_back = cv2.magnitude(img_back[:, :, 0], img_back[:, :, 1])\n",
    "\n",
    "    # Display original and low-pass filtered images side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Gray Original Image', 'image': gray_image},\n",
    "            {'title': 'Low-Pass Filtered Image', 'image': img_back}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Optionally save the filtered image\n",
    "    # cv2.imwrite('freq_lowpass.jpg', img_back)\n",
    "else:\n",
    "    print(\"No grayscale image loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Noise in Images: Addition and Removal\n",
    "\n",
    "Noise is an unwanted variation of intensity in images.  \n",
    "Common types include Gaussian, salt-and-pepper, and speckle noise.  \n",
    "\n",
    "This section demonstrates:\n",
    "\n",
    "1. Adding **Gaussian noise** to an image.\n",
    "2. Removing noise using **spatial filters** like Gaussian and bilateral filters.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.1 Adding Gaussian Noise\n",
    "\n",
    "Gaussian noise has a **normal distribution** with mean μ and standard deviation σ.  \n",
    "\n",
    "- Pixels are randomly perturbed:  \n",
    "\n",
    "$$\n",
    "I_{noisy}(x, y) = I(x, y) + n(x, y), \\quad n(x, y) \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "- Parameters `mean` and `sigma` control the intensity of the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image is not None:\n",
    "    mean = 0\n",
    "    var = 10\n",
    "    sigma = var ** 0.5\n",
    "    gaussian_noise = np.random.normal(mean, sigma, image.shape)\n",
    "    noisy_image = np.clip(image + gaussian_noise, 0, 255).astype(np.uint8)\n",
    "\n",
    "    denoised_gaussian = cv2.GaussianBlur(noisy_image, (5, 5), 0)\n",
    "    denoised_median = cv2.medianBlur(noisy_image, 5)\n",
    "\n",
    "\n",
    "    # Display original and low-pass filtered images side by side\n",
    "    learn_tools.show_multiple_images(\n",
    "        image_plotting_data=[\n",
    "            {'title': 'Original Image', 'image': image},\n",
    "            {'title': 'Noisy Image', 'image': noisy_image},\n",
    "            {'title': 'DeNoised Gaussian Image', 'image': denoised_gaussian},\n",
    "            {'title': 'DeNoised Median Image', 'image': denoised_median}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # cv2.imwrite('noisy.jpg', noisy_image)\n",
    "    # cv2.imwrite('denoised_gaussian.jpg', denoised_gaussian)\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"No image loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Considerations and Optimization\n",
    "\n",
    "- **Separable filters for speed:** Use filters like Gaussian that can be broken into 1D operations.\n",
    "- **Integral images for box filters:** Enables fast computation of sum over regions.\n",
    "- **GPU acceleration with OpenCV CUDA:** Leverage GPU for heavy image processing tasks.\n",
    "- **Benchmarking tip:** Larger kernels are slower; prefer built-in optimized functions whenever possible.\n",
    "\n",
    "### ✅ Notes:\n",
    "- `cv2.filter2D` applies a custom kernel; slower for large kernels.\n",
    "- `cv2.blur` is optimized; usually faster for standard box filters.\n",
    "- You can replace `(15,15)` with any kernel size to benchmark performance differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Make sure 'image' is loaded, e.g., image = cv2.imread(\"path_to_image.jpg\")\n",
    "if image is not None:\n",
    "    # Custom filter using filter2D\n",
    "    start = time.time()\n",
    "    kernel = np.ones((15, 15), np.float32) / 225\n",
    "    cv2.filter2D(image, -1, kernel)\n",
    "    custom_time = time.time() - start\n",
    "\n",
    "    # Built-in blur function\n",
    "    start = time.time()\n",
    "    cv2.blur(image, (15, 15))\n",
    "    builtin_time = time.time() - start\n",
    "\n",
    "    # Print performance comparison\n",
    "    print(f'Custom Filter Time: {custom_time:.4f}s')\n",
    "    print(f'Built-in Blur Time: {builtin_time:.4f}s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
